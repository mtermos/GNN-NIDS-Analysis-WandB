{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:21.407444Z",
                    "iopub.status.busy": "2024-08-24T12:01:21.407010Z",
                    "iopub.status.idle": "2024-08-24T12:01:29.827271Z",
                    "shell.execute_reply": "2024-08-24T12:01:29.826059Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:21.407404Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import pickle\n",
                "import random\n",
                "import socket\n",
                "import struct\n",
                "\n",
                "import networkx as nx\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "from scipy.sparse import csr_matrix\n",
                "\n",
                "from src.dataset.dataset_info import datasets\n",
                "from src.graph.graph_construction import create_weightless_window_graph\n",
                "from src.graph.graph_measures import calculate_graph_measures\n",
                "from src.graph.centralities import add_centralities, add_centralities_as_node_features\n",
                "from local_variables import local_datasets_path\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_class = True\n",
                "\n",
                "use_node_features = False\n",
                "\n",
                "use_port_in_address = False\n",
                "\n",
                "generated_ips = False\n",
                "\n",
                "# graph_type = \"flow\"\n",
                "graph_type = \"window\"\n",
                "# graph_type = \"line\"\n",
                "\n",
                "window_size= 500\n",
                "\n",
                "# sort_timestamp = False\n",
                "sort_timestamp = True\n",
                "\n",
                "# k_fold = None\n",
                "# k_fold = 5\n",
                "\n",
                "validation_size = 0.1\n",
                "test_size = 0.1\n",
                "\n",
                "cn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n",
                "# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n",
                "\n",
                "network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n",
                "# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# name = \"cic_ton_iot_5_percent\"\n",
                "# name = \"cic_ton_iot\"\n",
                "# name = \"cic_ids_2017_5_percent\"\n",
                "name = \"cic_ids_2017\"\n",
                "# name = \"cic_bot_iot\"\n",
                "# name = \"cic_ton_iot_modified\"\n",
                "# name = \"nf_ton_iotv2_modified\"\n",
                "# name = \"ccd_inid_modified\"\n",
                "# name = \"nf_uq_nids_modified\"\n",
                "# name = \"edge_iiot\"\n",
                "# name = \"nf_cse_cic_ids2018\"\n",
                "# name = \"nf_bot_iotv2\"\n",
                "# name = \"nf_uq_nids\"\n",
                "# name = \"x_iiot\"\n",
                "\n",
                "dataset = datasets[name]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'C:\\\\Users\\\\Administrateur\\\\Desktop\\\\datasets\\\\cic_ids_2017\\\\flow__multi_class__unsorted'"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "g_type = \"\"\n",
                "if graph_type == \"flow\":\n",
                "    g_type = \"flow\"\n",
                "elif graph_type == \"line\":\n",
                "    g_type = f\"line_graph_{window_size}\"\n",
                "elif graph_type == \"window\":\n",
                "    g_type = f\"window_graph_{window_size}\"\n",
                "    \n",
                "if multi_class:\n",
                "    g_type += \"__multi_class\"\n",
                "    \n",
                "if use_node_features:\n",
                "    g_type += \"__n_feats\"\n",
                "    \n",
                "# if k_fold:\n",
                "#     g_type += f\"__{k_fold}_fold\"\n",
                "    \n",
                "if use_port_in_address:\n",
                "    g_type += \"__ports\"\n",
                "    \n",
                "if generated_ips:\n",
                "    g_type += \"__generated_ips\"\n",
                "    \n",
                "if sort_timestamp:\n",
                "    g_type += \"__sorted\"\n",
                "else:\n",
                "    g_type += \"__unsorted\"\n",
                "    \n",
                "dataset_path = os.path.join(local_datasets_path,name)\n",
                "folder_path = os.path.join(dataset_path, g_type)\n",
                "# folder_path = f\"datasets/{name}/{g_type}\"\n",
                "folder_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:38.979760Z",
                    "iopub.status.busy": "2024-08-24T12:01:38.979252Z",
                    "iopub.status.idle": "2024-08-24T12:01:39.036289Z",
                    "shell.execute_reply": "2024-08-24T12:01:39.035076Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:38.979720Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Flow ID</th>\n",
                            "      <th>Src IP</th>\n",
                            "      <th>Src Port</th>\n",
                            "      <th>Dst IP</th>\n",
                            "      <th>Dst Port</th>\n",
                            "      <th>Protocol</th>\n",
                            "      <th>Timestamp</th>\n",
                            "      <th>Flow Duration</th>\n",
                            "      <th>Tot Fwd Pkts</th>\n",
                            "      <th>Tot Bwd Pkts</th>\n",
                            "      <th>...</th>\n",
                            "      <th>Active Std</th>\n",
                            "      <th>Active Max</th>\n",
                            "      <th>Active Min</th>\n",
                            "      <th>Idle Mean</th>\n",
                            "      <th>Idle Std</th>\n",
                            "      <th>Idle Max</th>\n",
                            "      <th>Idle Min</th>\n",
                            "      <th>Label</th>\n",
                            "      <th>Attack</th>\n",
                            "      <th>Class</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>index</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>192.168.10.5-104.16.207.165-54865-443-6</td>\n",
                            "      <td>104.16.207.165</td>\n",
                            "      <td>443.0</td>\n",
                            "      <td>192.168.10.5</td>\n",
                            "      <td>54865.0</td>\n",
                            "      <td>6.0</td>\n",
                            "      <td>7/7/2017 3:30</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>2.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>BENIGN</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>192.168.10.5-104.16.28.216-55054-80-6</td>\n",
                            "      <td>104.16.28.216</td>\n",
                            "      <td>80.0</td>\n",
                            "      <td>192.168.10.5</td>\n",
                            "      <td>55054.0</td>\n",
                            "      <td>6.0</td>\n",
                            "      <td>7/7/2017 3:30</td>\n",
                            "      <td>109.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>BENIGN</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>192.168.10.5-104.16.28.216-55055-80-6</td>\n",
                            "      <td>104.16.28.216</td>\n",
                            "      <td>80.0</td>\n",
                            "      <td>192.168.10.5</td>\n",
                            "      <td>55055.0</td>\n",
                            "      <td>6.0</td>\n",
                            "      <td>7/7/2017 3:30</td>\n",
                            "      <td>52.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>BENIGN</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>192.168.10.16-104.17.241.25-46236-443-6</td>\n",
                            "      <td>104.17.241.25</td>\n",
                            "      <td>443.0</td>\n",
                            "      <td>192.168.10.16</td>\n",
                            "      <td>46236.0</td>\n",
                            "      <td>6.0</td>\n",
                            "      <td>7/7/2017 3:30</td>\n",
                            "      <td>34.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>1.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>BENIGN</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>192.168.10.5-104.19.196.102-54863-443-6</td>\n",
                            "      <td>104.19.196.102</td>\n",
                            "      <td>443.0</td>\n",
                            "      <td>192.168.10.5</td>\n",
                            "      <td>54863.0</td>\n",
                            "      <td>6.0</td>\n",
                            "      <td>7/7/2017 3:30</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>2.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0.0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>BENIGN</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows × 85 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                       Flow ID          Src IP Src Port  \\\n",
                            "index                                                                     \n",
                            "0      192.168.10.5-104.16.207.165-54865-443-6  104.16.207.165    443.0   \n",
                            "1        192.168.10.5-104.16.28.216-55054-80-6   104.16.28.216     80.0   \n",
                            "2        192.168.10.5-104.16.28.216-55055-80-6   104.16.28.216     80.0   \n",
                            "3      192.168.10.16-104.17.241.25-46236-443-6   104.17.241.25    443.0   \n",
                            "4      192.168.10.5-104.19.196.102-54863-443-6  104.19.196.102    443.0   \n",
                            "\n",
                            "              Dst IP Dst Port  Protocol      Timestamp  Flow Duration  \\\n",
                            "index                                                                   \n",
                            "0       192.168.10.5  54865.0       6.0  7/7/2017 3:30            3.0   \n",
                            "1       192.168.10.5  55054.0       6.0  7/7/2017 3:30          109.0   \n",
                            "2       192.168.10.5  55055.0       6.0  7/7/2017 3:30           52.0   \n",
                            "3      192.168.10.16  46236.0       6.0  7/7/2017 3:30           34.0   \n",
                            "4       192.168.10.5  54863.0       6.0  7/7/2017 3:30            3.0   \n",
                            "\n",
                            "       Tot Fwd Pkts  Tot Bwd Pkts  ...  Active Std  Active Max  Active Min  \\\n",
                            "index                              ...                                       \n",
                            "0               2.0           0.0  ...         0.0         0.0         0.0   \n",
                            "1               1.0           1.0  ...         0.0         0.0         0.0   \n",
                            "2               1.0           1.0  ...         0.0         0.0         0.0   \n",
                            "3               1.0           1.0  ...         0.0         0.0         0.0   \n",
                            "4               2.0           0.0  ...         0.0         0.0         0.0   \n",
                            "\n",
                            "       Idle Mean  Idle Std  Idle Max  Idle Min  Label  Attack  Class  \n",
                            "index                                                                 \n",
                            "0            0.0       0.0       0.0       0.0      0  BENIGN      0  \n",
                            "1            0.0       0.0       0.0       0.0      0  BENIGN      0  \n",
                            "2            0.0       0.0       0.0       0.0      0  BENIGN      0  \n",
                            "3            0.0       0.0       0.0       0.0      0  BENIGN      0  \n",
                            "4            0.0       0.0       0.0       0.0      0  BENIGN      0  \n",
                            "\n",
                            "[5 rows x 85 columns]"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:39.037975Z",
                    "iopub.status.busy": "2024-08-24T12:01:39.037631Z",
                    "iopub.status.idle": "2024-08-24T12:01:39.067624Z",
                    "shell.execute_reply": "2024-08-24T12:01:39.066190Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:39.037946Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Label\n",
                            "0    2265910\n",
                            "1     548968\n",
                            "Name: count, dtype: int64"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df[dataset.label_col].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'mixed'"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset.timestamp_format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "dtype('O')"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df[dataset.timestamp_col].dtypes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "ename": "MemoryError",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[12], line 63\u001b[0m\n\u001b[0;32m     57\u001b[0m         G\u001b[39m.\u001b[39madd_edges_from(generate_edges_for_group(indices, times, time_window_seconds))\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m G\n\u001b[1;32m---> 63\u001b[0m G_time \u001b[39m=\u001b[39m method_grouping_time_generator(df, time_window_seconds\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, timestamp_col\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mtimestamp_col, src_ip_col\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49msrc_ip_col)\n\u001b[0;32m     64\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGraph created by the grouping method:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNodes:\u001b[39m\u001b[39m\"\u001b[39m, G_time\u001b[39m.\u001b[39mnumber_of_nodes())\n",
                        "Cell \u001b[1;32mIn[12], line 57\u001b[0m, in \u001b[0;36mmethod_grouping_time_generator\u001b[1;34m(df, time_window_seconds, timestamp_col, src_ip_col)\u001b[0m\n\u001b[0;32m     54\u001b[0m     times \u001b[39m=\u001b[39m (group_sorted[timestamp_col]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m9\u001b[39m)\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Add edges generated by the generator function.\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     G\u001b[39m.\u001b[39;49madd_edges_from(generate_edges_for_group(indices, times, time_window_seconds))\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m G\n",
                        "File \u001b[1;32mc:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\networkx\\classes\\graph.py:1045\u001b[0m, in \u001b[0;36mGraph.add_edges_from\u001b[1;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     datadict\u001b[39m.\u001b[39mupdate(attr)\n\u001b[0;32m   1044\u001b[0m     datadict\u001b[39m.\u001b[39mupdate(dd)\n\u001b[1;32m-> 1045\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adj[u][v] \u001b[39m=\u001b[39m datadict\n\u001b[0;32m   1046\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adj[v][u] \u001b[39m=\u001b[39m datadict\n\u001b[0;32m   1047\u001b[0m nx\u001b[39m.\u001b[39m_clear_cache(\u001b[39mself\u001b[39m)\n",
                        "\u001b[1;31mMemoryError\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "def method_grouping_time_generator(df, time_window_seconds=60, timestamp_col=\"timestamp\", src_ip_col = \"src_ip\"):\n",
                "    \"\"\"\n",
                "    Create a flow graph where each node represents a flow (a row in df) and an edge is added\n",
                "    between two flows if:\n",
                "      1. They share the same source IP address.\n",
                "      2. Their timestamps are within time_window_seconds.\n",
                "    \n",
                "    It converts the \"timestamp\" column to datetime and uses a generator to stream\n",
                "    edge creation in order to save memory.\n",
                "    \n",
                "    Parameters:\n",
                "      df (pd.DataFrame): Flow dataset.\n",
                "      time_window_seconds (int): Maximum allowed time difference between flows (in seconds).\n",
                "    \n",
                "    Returns:\n",
                "      G (networkx.Graph): Graph with nodes representing flows and edges connecting flows\n",
                "                          with matching src_ip that occur within the given time window.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Convert the timestamp column to datetime if not already\n",
                "    if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):\n",
                "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], format='mixed')\n",
                "    \n",
                "    # Create an empty graph and add each flow as a node with its attributes.\n",
                "    G = nx.Graph()\n",
                "    for idx, row in df.iterrows():\n",
                "        G.add_node(idx, **row.to_dict())\n",
                "    \n",
                "    def generate_edges_for_group(indices, times, window):\n",
                "        \"\"\"\n",
                "        Generator that yields an edge for each pair of flows in the same group\n",
                "        where the time difference is within the window.\n",
                "        \n",
                "        Parameters:\n",
                "          indices: numpy array of original DataFrame indices for the group.\n",
                "          times: numpy array of the corresponding timestamps (in seconds).\n",
                "          window: maximum allowed time difference (in seconds).\n",
                "        \"\"\"\n",
                "        n = len(times)\n",
                "        for i in range(n):\n",
                "            # For the i-th flow, find the right boundary index where the timestamp\n",
                "            # is greater than times[i] + window.\n",
                "            right_bound = np.searchsorted(times, times[i] + window, side='right')\n",
                "            # Yield an edge from the current node to all nodes between i+1 and right_bound.\n",
                "            for j in range(i + 1, right_bound):\n",
                "                yield (indices[i], indices[j])\n",
                "    \n",
                "    # Group the DataFrame by source IP and process each group.\n",
                "    for src_ip, group in df.groupby(src_ip_col):\n",
                "        # Sort flows by timestamp.\n",
                "        group_sorted = group.sort_values(timestamp_col)\n",
                "        indices = group_sorted.index.to_numpy()\n",
                "        # Convert timestamps to seconds since epoch (as int64 division)\n",
                "        times = (group_sorted[timestamp_col].astype(np.int64) // 10**9).to_numpy()\n",
                "        \n",
                "        # Add edges generated by the generator function.\n",
                "        G.add_edges_from(generate_edges_for_group(indices, times, time_window_seconds))\n",
                "    \n",
                "    return G\n",
                "\n",
                "\n",
                "\n",
                "G_time = method_grouping_time_generator(df, time_window_seconds=0, timestamp_col=dataset.timestamp_col, src_ip_col=dataset.src_ip_col)\n",
                "print(\"Graph created by the grouping method:\")\n",
                "print(\"Nodes:\", G_time.number_of_nodes())\n",
                "print(\"Edges:\", G_time.number_of_edges())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Graph created by the grouping method:\n",
                        "Nodes: 692543\n",
                        "Edges: 0\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import networkx as nx\n",
                "\n",
                "def method_grouping_time(df, time_window_seconds=60):\n",
                "    \"\"\"\n",
                "    Create a flow graph where each node represents a flow (a row in df) and an edge is added\n",
                "    between two flows if:\n",
                "      1. They share the same source IP address.\n",
                "      2. Their timestamps are within `time_window_hours` hours.\n",
                "    \n",
                "    Assumes the DataFrame `df` has at least the following columns:\n",
                "      - \"src_ip\": source IP address for the flow.\n",
                "      - \"timestamp\": a column of type object representing timestamps.\n",
                "    \n",
                "    This function will convert the \"timestamp\" column to datetime.\n",
                "    \n",
                "    Parameters:\n",
                "      df (pd.DataFrame): Flow dataset with \"src_ip\" and \"timestamp\" columns.\n",
                "      time_window_hours (float): Maximum time difference (in hours) between two flows \n",
                "                                 for an edge to be added.\n",
                "    \n",
                "    Returns:\n",
                "      G (networkx.Graph): Graph where each node represents a flow (indexed by DataFrame row index)\n",
                "                          and an edge exists if both flows share the same src_ip and their \n",
                "                          timestamp difference is within the specified time window.\n",
                "    \"\"\"\n",
                "    # Convert the timestamp column to datetime if not already\n",
                "    if not pd.api.types.is_datetime64_any_dtype(df[dataset.timestamp_col]):\n",
                "        df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col], format=dataset.timestamp_format)\n",
                "    \n",
                "    # Create an empty graph\n",
                "    G = nx.Graph()\n",
                "    \n",
                "    # Add every flow as a node with its attributes.\n",
                "    for idx, row in df.iterrows():\n",
                "        G.add_node(idx, **row.to_dict())\n",
                "    \n",
                "    \n",
                "    # Group the flows by 'src_ip'\n",
                "    for src_ip, group in df.groupby(dataset.src_ip_col):\n",
                "        # Sort the group by timestamp.\n",
                "        group_sorted = group.sort_values(dataset.timestamp_col)\n",
                "        # Get the list of original indices and their corresponding timestamps.\n",
                "        indices = group_sorted.index.tolist()\n",
                "        timestamps = group_sorted[dataset.timestamp_col].tolist()\n",
                "        \n",
                "        # For each flow, add edges to subsequent flows in the sorted group while their \n",
                "        # difference in time is within the allowed window.\n",
                "        n = len(timestamps)\n",
                "        for i in range(n):\n",
                "            j = i + 1\n",
                "            while j < n and (timestamps[j] - timestamps[i]).total_seconds() <= time_window_seconds:\n",
                "                G.add_edge(indices[i], indices[j])\n",
                "                j += 1\n",
                "                \n",
                "    return G\n",
                "\n",
                "G_time = method_grouping_time(df, time_window_seconds=1)\n",
                "print(\"Graph created by the grouping method:\")\n",
                "print(\"Nodes:\", G_time.number_of_nodes())\n",
                "print(\"Edges:\", G_time.number_of_edges())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "def method_grouping(df):\n",
                "    \"\"\"\n",
                "    Create a flow graph by grouping flows based on their source IP.\n",
                "    \n",
                "    For each source IP, every pair of flows is connected with an edge.\n",
                "    \n",
                "    Parameters:\n",
                "        df (pd.DataFrame): DataFrame containing flow data with at least a \"src_ip\" column.\n",
                "        \n",
                "    Returns:\n",
                "        G (networkx.Graph): Graph where each node is a flow (row index) and\n",
                "                            an edge exists if two flows share the same source IP.\n",
                "    \"\"\"\n",
                "    G = nx.Graph()\n",
                "    # Add each flow as a node with its attributes.\n",
                "    for idx, row in df.iterrows():\n",
                "        G.add_node(idx, **row.to_dict())\n",
                "    \n",
                "    # Group the flows by 'src_ip' for fast lookup of flows with the same source.\n",
                "    grouped = df.groupby(dataset.src_ip_col).indices\n",
                "    \n",
                "    # For each source IP group, add an edge between every pair of flows.\n",
                "    for src, indices in grouped.items():\n",
                "        if len(indices) < 2:\n",
                "            continue  # No edge to add if only one flow for this src_ip.\n",
                "        for i in range(len(indices)):\n",
                "            for j in range(i + 1, len(indices)):\n",
                "                G.add_edge(indices[i], indices[j])\n",
                "    \n",
                "    return G\n",
                "\n",
                "\n",
                "def method_bipartite(df):\n",
                "    \"\"\"\n",
                "    Create a flow graph using a bipartite approach and then projecting onto flows.\n",
                "    \n",
                "    The bipartite graph has:\n",
                "        - Flow nodes (each flow with its attributes)\n",
                "        - Source IP nodes\n",
                "    An edge connects a flow node to its source IP node.\n",
                "    \n",
                "    Then, the bipartite graph is projected onto the flow nodes. That is,\n",
                "    two flow nodes share an edge if they both connect to the same source IP.\n",
                "    \n",
                "    Parameters:\n",
                "        df (pd.DataFrame): DataFrame containing flow data with at least a \"src_ip\" column.\n",
                "        \n",
                "    Returns:\n",
                "        G (networkx.Graph): Flow graph where nodes (original flow indices) are connected if\n",
                "                            the flows share the same source IP.\n",
                "    \"\"\"\n",
                "    B = nx.Graph()\n",
                "    # Add flow nodes: prefix the node id to avoid conflict with source IPs.\n",
                "    for idx, row in df.iterrows():\n",
                "        B.add_node(f'flow_{idx}', bipartite=0, **row.to_dict())\n",
                "    # Add source IP nodes.\n",
                "    src_ips = df[dataset.src_ip_col].unique()\n",
                "    for src in src_ips:\n",
                "        B.add_node(src, bipartite=1)\n",
                "    \n",
                "    # Create bipartite edges from each flow to its corresponding source IP.\n",
                "    for idx, row in df.iterrows():\n",
                "        B.add_edge(f'flow_{idx}', row[dataset.src_ip_col])\n",
                "    \n",
                "    # Project the bipartite graph onto flow nodes.\n",
                "    flow_nodes = [n for n, d in B.nodes(data=True) if d.get('bipartite') == 0]\n",
                "    G = nx.algorithms.bipartite.projected_graph(B, flow_nodes)\n",
                "    \n",
                "    # (Optional) Relabel nodes to recover the original integer indices.\n",
                "    mapping = {node: int(node.replace('flow_', '')) for node in flow_nodes}\n",
                "    G = nx.relabel_nodes(G, mapping)\n",
                "    \n",
                "    return G\n",
                "\n",
                "# Method 1: Grouping Method\n",
                "G_grouping = method_grouping(df)\n",
                "print(\"Graph created by the grouping method:\")\n",
                "print(\"Nodes:\", G_grouping.number_of_nodes())\n",
                "print(\"Edges:\", G_grouping.number_of_edges())\n",
                "\n",
                "# Method 2: Bipartite Projection Method\n",
                "G_bipartite = method_bipartite(df)\n",
                "print(\"\\nGraph created by the bipartite method:\")\n",
                "print(\"Nodes:\", G_bipartite.number_of_nodes())\n",
                "print(\"Edges:\", G_bipartite.number_of_edges())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "if multi_class:\n",
                "    y = df[dataset.class_num_col]\n",
                "else:\n",
                "    y = df[dataset.label_col]\n",
                "\n",
                "if sort_timestamp:\n",
                "    X_tr, X_test, y_tr, y_test = train_test_split(\n",
                "        df, y, test_size=test_size)\n",
                "    \n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_tr, y_tr, test_size=validation_size)\n",
                "else:\n",
                "    X_tr, X_test, y_tr, y_test = train_test_split(\n",
                "        df, y, test_size=test_size, random_state=13, stratify=y)\n",
                "    \n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n",
                "\n",
                "del df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"line\" and use_node_features:\n",
                "    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "X_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\n",
                "X_train['h'] = X_train[ cols_to_norm ].values.tolist()\n",
                "\n",
                "cols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\n",
                "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
                "\n",
                "X_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\n",
                "X_val['h'] = X_val[ cols_to_norm ].values.tolist()\n",
                "X_val.drop(cols_to_drop, axis=1, inplace=True)\n",
                "\n",
                "X_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\n",
                "X_test['h'] = X_test[ cols_to_norm ].values.tolist()\n",
                "X_test.drop(cols_to_drop, axis=1, inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"window\" or graph_type == \"line\":\n",
                "\n",
                "    create_weightless_window_graph(\n",
                "        df=X_train,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"training\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")\n",
                "    \n",
                "    create_weightless_window_graph(\n",
                "        df=X_val,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"validation\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")\n",
                "    \n",
                "    create_weightless_window_graph(\n",
                "        df=X_test,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"testing\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:58.208317Z",
                    "iopub.status.busy": "2024-08-24T12:01:58.207922Z",
                    "iopub.status.idle": "2024-08-24T12:02:01.575513Z",
                    "shell.execute_reply": "2024-08-24T12:02:01.574335Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:58.208283Z"
                },
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==>> X_train.shape: (511768, 5)\n",
                        "==>> X_val.shape: (56864, 5)\n",
                        "==>> X_test.shape: (63182, 5)\n"
                    ]
                }
            ],
            "source": [
                "if graph_type == \"flow\":\n",
                "\tos.makedirs(folder_path, exist_ok=True)\n",
                "\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n",
                "\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n",
                "\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==>> calculated degrees, in 0.0001368001103401184 seconds\n",
                        "==>> graph_measures: {'number_of_nodes': 107, 'number_of_edges': 511768, 'max_degree': 238453, 'avg_degree': 9565.757009345794, 'density': 45.12149532710281}\n"
                    ]
                }
            ],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"training_graph\"\n",
                "\n",
                "    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    # get netowrk properties\n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==>> calculated degrees, in 0.00010430067777633667 seconds\n",
                        "==>> graph_measures: {'number_of_nodes': 78, 'number_of_edges': 56864, 'max_degree': 26533, 'avg_degree': 1458.051282051282, 'density': 9.467865467865467}\n"
                    ]
                }
            ],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"validation_graph\"\n",
                "\n",
                "    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    # get netowrk properties\n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==>> calculated degrees, in 0.00010609999299049377 seconds\n",
                        "==>> graph_measures: {'number_of_nodes': 80, 'number_of_edges': 63182, 'max_degree': 29310, 'avg_degree': 1579.55, 'density': 9.997151898734177}\n"
                    ]
                }
            ],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"testing_graph\"\n",
                "    \n",
                "    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "    \n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "    \n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "datasetId": 4775518,
                    "sourceId": 8089266,
                    "sourceType": "datasetVersion"
                },
                {
                    "datasetId": 4775527,
                    "sourceId": 8089281,
                    "sourceType": "datasetVersion"
                }
            ],
            "isGpuEnabled": false,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
