{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:21.407444Z",
                    "iopub.status.busy": "2024-08-24T12:01:21.407010Z",
                    "iopub.status.idle": "2024-08-24T12:01:29.827271Z",
                    "shell.execute_reply": "2024-08-24T12:01:29.826059Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:21.407404Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import pickle\n",
                "import random\n",
                "import socket\n",
                "import struct\n",
                "\n",
                "import networkx as nx\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "from src.dataset.dataset_info import datasets\n",
                "from src.graph.graph_construction import create_weightless_window_graph\n",
                "from src.graph.graph_measures import calculate_graph_measures\n",
                "from src.graph.centralities import add_centralities, add_centralities_as_node_features\n",
                "from local_variables import local_datasets_path\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "multi_class = True\n",
                "\n",
                "use_node_features = False\n",
                "\n",
                "use_port_in_address = False\n",
                "\n",
                "generated_ips = False\n",
                "\n",
                "graph_type = \"flow\"\n",
                "# graph_type = \"window\"\n",
                "# graph_type = \"line\"\n",
                "\n",
                "window_size= 500\n",
                "\n",
                "sort_timestamp = False\n",
                "\n",
                "# k_fold = None\n",
                "# k_fold = 5\n",
                "\n",
                "validation_size = 0.1\n",
                "test_size = 0.1\n",
                "\n",
                "cn_measures = [\"betweenness\", \"degree\", \"pagerank\", \"closeness\", \"k_truss\"]\n",
                "# cn_measures = [\"betweenness\", \"degree\", \"closeness\"]\n",
                "\n",
                "network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank', 'src_closeness', 'dst_closeness', 'src_k_truss', 'dst_k_truss']\n",
                "# network_features = ['src_betweenness', 'dst_betweenness', 'src_degree', 'dst_degree', 'src_pagerank', 'dst_pagerank']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# name = \"cic_ton_iot_5_percent\"\n",
                "# name = \"cic_ton_iot\"\n",
                "name = \"cic_ids_2017_5_percent\"\n",
                "# name = \"cic_ids_2017\"\n",
                "# name = \"cic_bot_iot\"\n",
                "# name = \"cic_ton_iot_modified\"\n",
                "# name = \"nf_ton_iotv2_modified\"\n",
                "# name = \"ccd_inid_modified\"\n",
                "# name = \"nf_uq_nids_modified\"\n",
                "# name = \"edge_iiot\"\n",
                "# name = \"nf_cse_cic_ids2018\"\n",
                "# name = \"nf_bot_iotv2\"\n",
                "# name = \"nf_uq_nids\"\n",
                "# name = \"x_iiot\"\n",
                "\n",
                "dataset = datasets[name]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "g_type = \"\"\n",
                "if graph_type == \"flow\":\n",
                "    g_type = \"flow\"\n",
                "elif graph_type == \"line\":\n",
                "    g_type = f\"line_graph_{window_size}\"\n",
                "elif graph_type == \"window\":\n",
                "    g_type = f\"window_graph_{window_size}\"\n",
                "    \n",
                "if multi_class:\n",
                "    g_type += \"__multi_class\"\n",
                "    \n",
                "if use_node_features:\n",
                "    g_type += \"__n_feats\"\n",
                "    \n",
                "# if k_fold:\n",
                "#     g_type += f\"__{k_fold}_fold\"\n",
                "    \n",
                "if use_port_in_address:\n",
                "    g_type += \"__ports\"\n",
                "    \n",
                "if generated_ips:\n",
                "    g_type += \"__generated_ips\"\n",
                "    \n",
                "if sort_timestamp:\n",
                "    g_type += \"__sorted\"\n",
                "else:\n",
                "    g_type += \"__unsorted\"\n",
                "    \n",
                "dataset_path = os.path.join(local_datasets_path,name)\n",
                "folder_path = os.path.join(dataset_path, g_type)\n",
                "# folder_path = f\"datasets/{name}/{g_type}\"\n",
                "folder_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_parquet(os.path.join(dataset_path, f\"{name}.parquet\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:38.979760Z",
                    "iopub.status.busy": "2024-08-24T12:01:38.979252Z",
                    "iopub.status.idle": "2024-08-24T12:01:39.036289Z",
                    "shell.execute_reply": "2024-08-24T12:01:39.035076Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:38.979720Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "cols_to_norm = list(set(list(df.columns))  - set(list([dataset.label_col, dataset.class_num_col])) - set(dataset.drop_columns)  - set(dataset.weak_columns))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:39.037975Z",
                    "iopub.status.busy": "2024-08-24T12:01:39.037631Z",
                    "iopub.status.idle": "2024-08-24T12:01:39.067624Z",
                    "shell.execute_reply": "2024-08-24T12:01:39.066190Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:39.037946Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "df[dataset.label_col].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "if generated_ips:\n",
                "    df[dataset.src_ip_col] = df[dataset.src_ip_col].apply(lambda x: socket.inet_ntoa(struct.pack('>I', random.randint(0xac100001, 0xac1f0001))))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "if sort_timestamp:\n",
                "    df[dataset.timestamp_col] = pd.to_datetime(df[dataset.timestamp_col].str.strip(), format=dataset.timestamp_format)\n",
                "    df.sort_values(dataset.timestamp_col, inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "if use_port_in_address:\n",
                "    df[dataset.src_port_col] = df[dataset.src_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
                "    df[dataset.src_ip_col] = df[dataset.src_ip_col] + ':' + df[dataset.src_port_col]\n",
                "\n",
                "    df[dataset.dst_port_col] = df[dataset.dst_port_col].astype(float).astype(int).astype(str) # to remove the decimal point\n",
                "    df[dataset.dst_ip_col] = df[dataset.dst_ip_col] + ':' + df[dataset.dst_port_col]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "if multi_class:\n",
                "    y = df[dataset.class_num_col]\n",
                "else:\n",
                "    y = df[dataset.label_col]\n",
                "\n",
                "if sort_timestamp:\n",
                "    X_tr, X_test, y_tr, y_test = train_test_split(\n",
                "        df, y, test_size=test_size)\n",
                "    \n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_tr, y_tr, test_size=validation_size)\n",
                "else:\n",
                "    X_tr, X_test, y_tr, y_test = train_test_split(\n",
                "        df, y, test_size=test_size, random_state=13, stratify=y)\n",
                "    \n",
                "    X_train, X_val, y_train, y_val = train_test_split(\n",
                "        X_tr, y_tr, test_size=validation_size, random_state=13, stratify=y_tr)\n",
                "\n",
                "del df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"line\" and use_node_features:\n",
                "    add_centralities(df = X_train, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    add_centralities(df = X_val, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    add_centralities(df = X_test, new_path=None, graph_path=None, dataset=dataset, cn_measures=cn_measures, network_features=network_features, create_using=nx.MultiDiGraph())\n",
                "    cols_to_norm = list(set(cols_to_norm) | set(network_features))\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "X_train[cols_to_norm] = scaler.fit_transform(X_train[cols_to_norm])\n",
                "X_train['h'] = X_train[ cols_to_norm ].values.tolist()\n",
                "\n",
                "cols_to_drop = list(set(list(X_train.columns)) - set(list([dataset.label_col, dataset.src_ip_col, dataset.dst_ip_col, dataset.class_num_col, 'h'])))\n",
                "X_train.drop(cols_to_drop, axis=1, inplace=True)\n",
                "\n",
                "X_val[cols_to_norm] = scaler.transform(X_val[cols_to_norm])\n",
                "X_val['h'] = X_val[ cols_to_norm ].values.tolist()\n",
                "X_val.drop(cols_to_drop, axis=1, inplace=True)\n",
                "\n",
                "X_test[cols_to_norm] = scaler.transform(X_test[cols_to_norm])\n",
                "X_test['h'] = X_test[ cols_to_norm ].values.tolist()\n",
                "X_test.drop(cols_to_drop, axis=1, inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"window\" or graph_type == \"line\":\n",
                "\n",
                "    create_weightless_window_graph(\n",
                "        df=X_train,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"training\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")\n",
                "    \n",
                "    create_weightless_window_graph(\n",
                "        df=X_val,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"validation\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")\n",
                "    \n",
                "    create_weightless_window_graph(\n",
                "        df=X_test,\n",
                "        dataset=dataset,\n",
                "        window_size=window_size,\n",
                "        line_graph=graph_type == \"line\",\n",
                "        folder_path=os.path.join(folder_path, \"testing\"),\n",
                "        edge_attr= ['h', dataset.label_col, dataset.class_num_col],\n",
                "        file_type=\"pkl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2024-08-24T12:01:58.208317Z",
                    "iopub.status.busy": "2024-08-24T12:01:58.207922Z",
                    "iopub.status.idle": "2024-08-24T12:02:01.575513Z",
                    "shell.execute_reply": "2024-08-24T12:02:01.574335Z",
                    "shell.execute_reply.started": "2024-08-24T12:01:58.208283Z"
                },
                "trusted": true
            },
            "outputs": [],
            "source": [
                "if graph_type == \"flow\":\n",
                "\tos.makedirs(folder_path, exist_ok=True)\n",
                "\tprint(f\"==>> X_train.shape: {X_train.shape}\")\n",
                "\tprint(f\"==>> X_val.shape: {X_val.shape}\")\n",
                "\tprint(f\"==>> X_test.shape: {X_test.shape}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"training_graph\"\n",
                "\n",
                "    G = nx.from_pandas_edgelist(X_train, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    # get netowrk properties\n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"validation_graph\"\n",
                "\n",
                "    G = nx.from_pandas_edgelist(X_val, dataset.src_ip_col, dataset.dst_ip_col, ['h',dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    # get netowrk properties\n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G), \"datasets/\" + name + \"/training_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "\n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if graph_type == \"flow\":\n",
                "    graph_name = \"testing_graph\"\n",
                "    \n",
                "    G = nx.from_pandas_edgelist(X_test, dataset.src_ip_col, dataset.dst_ip_col, ['h', dataset.label_col, dataset.class_num_col], create_using=nx.MultiDiGraph())\n",
                "    \n",
                "    if use_node_features:\n",
                "        add_centralities_as_node_features(df=None, G=G, graph_path=None, dataset=dataset, cn_measures=cn_measures)\n",
                "        \n",
                "        for node in G.nodes():\n",
                "            centralities = []\n",
                "            for centrality in cn_measures:\n",
                "                centralities.append(G.nodes[node].get(centrality, 0)) # Default to 0 if missing\n",
                "                \n",
                "                # Combine features into a single vector\n",
                "            n_feats = np.array(centralities, dtype=np.float32)\n",
                "            \n",
                "            # Add the new feature to the node\n",
                "            G.nodes[node][\"n_feats\"] = n_feats\n",
                "            \n",
                "    graph_measures = calculate_graph_measures(G, f\"{folder_path}/{graph_name}_measures.json\", verbose=True)\n",
                "    print(f\"==>> graph_measures: {graph_measures}\")\n",
                "    \n",
                "    # graph_measures = calculate_graph_measures(nx.DiGraph(G_test), \"datasets/\" + name + \"/testing_graph_simple_measures.json\", verbose=True)\n",
                "    # print(f\"==>> graph_measures: {graph_measures}\")\n",
                "    \n",
                "    with open(f\"{folder_path}/{graph_name}.pkl\", \"wb\") as f:\n",
                "        pickle.dump(G, f)"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "none",
            "dataSources": [
                {
                    "datasetId": 4775518,
                    "sourceId": 8089266,
                    "sourceType": "datasetVersion"
                },
                {
                    "datasetId": 4775527,
                    "sourceId": 8089281,
                    "sourceType": "datasetVersion"
                }
            ],
            "isGpuEnabled": false,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
