{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import wandb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from src.models import EGAT, EGRAPHSAGE\n",
    "# from src.models import EGAT, EGCN, EGRAPHSAGE\n",
    "from src.lightning_model import GraphModel\n",
    "from src.lightning_data import GraphDataModule\n",
    "from src.dataset.dataset_info import datasets\n",
    "from local_variables import local_datasets_path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_wandb = False\n",
    "save_top_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datasets = [\n",
    "    datasets[\"cic_ton_iot\"],\n",
    "    datasets[\"cic_ids_2017\"],\n",
    "    datasets[\"cic_ton_iot_modified\"],\n",
    "    datasets[\"ccd_inid_modified\"],\n",
    "    datasets[\"nf_uq_nids_modified\"],\n",
    "    datasets[\"edge_iiot\"],\n",
    "    datasets[\"nf_cse_cic_ids2018\"],\n",
    "    datasets[\"nf_uq_nids\"],\n",
    "    datasets[\"x_iiot\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = max_epochs = 1\n",
    "# early_stopping_patience = 20\n",
    "learning_rate = 0.005\n",
    "weight_decay = 0.0\n",
    "ndim_out = [128, 128]\n",
    "num_layers = 2\n",
    "number_neighbors = [25, 10]\n",
    "activation = F.relu\n",
    "dropout = 0.0\n",
    "residual = True\n",
    "multi_class = True\n",
    "use_centralities_nfeats = False\n",
    "aggregation = \"mean\"\n",
    "\n",
    "run_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "g_type = \"flow\"\n",
    "\n",
    "if multi_class:\n",
    "    g_type += \"__multi_class\"\n",
    "\n",
    "if use_centralities_nfeats:\n",
    "    g_type += \"__n_feats\"\n",
    "\n",
    "g_type += \"__unsorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed_dict = {}\n",
    "\n",
    "for dataset in my_datasets:\n",
    "    dataset_folder = os.path.join(local_datasets_path, dataset.name)\n",
    "    graphs_folder = os.path.join(dataset_folder, g_type)\n",
    "\n",
    "    logs_folder = os.path.join(\"logs\", dataset.name)\n",
    "    os.makedirs(logs_folder, exist_ok=True)\n",
    "    wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n",
    "    os.makedirs(wandb_runs_path, exist_ok=True)\n",
    "\n",
    "    labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n",
    "    num_classes = 2\n",
    "    if multi_class:\n",
    "        with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n",
    "            labels_names = pickle.load(f)\n",
    "        labels_mapping = labels_names[0]\n",
    "    num_classes = len(labels_mapping)\n",
    "\n",
    "    dataset_kwargs = dict(\n",
    "        use_node_features=use_centralities_nfeats,\n",
    "        multi_class=True,\n",
    "        using_masking=False,\n",
    "        masked_class=2,\n",
    "        num_workers=0,\n",
    "        label_col=dataset.label_col,\n",
    "        class_num_col=dataset.class_num_col,\n",
    "        device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    data_module = GraphDataModule(\n",
    "        graphs_folder, batch_size=1, **dataset_kwargs)\n",
    "    data_module.setup()\n",
    "\n",
    "    ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n",
    "    edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n",
    "\n",
    "    my_models = {\n",
    "        # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n",
    "        #               dropout, residual, num_classes),\n",
    "        f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                                 residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n",
    "        f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                                             residual, num_classes, num_neighbors=None, aggregation=aggregation),\n",
    "        \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                    residual, num_classes, num_neighbors=None),\n",
    "        # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "        #                        residual, num_classes, num_neighbors=number_neighbors),\n",
    "    }\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n",
    "\n",
    "    elapsed = {}\n",
    "\n",
    "    for model_name, model in my_models.items():\n",
    "\n",
    "        config = {\n",
    "            \"run_dtime\": run_dtime,\n",
    "            \"type\": \"GNN\",\n",
    "            \"model_name\": model_name,\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"ndim\": ndim,\n",
    "            \"edim\": edim,\n",
    "            \"ndim_out\": ndim_out,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"number_neighbors\": number_neighbors,\n",
    "            \"activation\": activation.__name__,\n",
    "            \"dropout\": dropout,\n",
    "            \"residual\": residual,\n",
    "            \"multi_class\": multi_class,\n",
    "            \"aggregation\": aggregation,\n",
    "            # \"details\": \"updating edge features\",\n",
    "            \"early_stopping_patience\": early_stopping_patience,\n",
    "            \"use_centralities_nfeats\": use_centralities_nfeats,\n",
    "        }\n",
    "\n",
    "        graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n",
    "                                    labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True, verbose=False)\n",
    "\n",
    "        if using_wandb:\n",
    "            wandb_logger = WandbLogger(\n",
    "                project=f\"GNN-Analysis-{dataset.name}\",\n",
    "                name=model_name,\n",
    "                config=config,\n",
    "                save_dir=wandb_runs_path\n",
    "            )\n",
    "        else:\n",
    "            wandb_logger = None\n",
    "\n",
    "        f1_checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"val_f1_score\",\n",
    "            mode=\"max\",\n",
    "            filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n",
    "            save_top_k=save_top_k,\n",
    "            save_on_train_epoch_end=False,\n",
    "            verbose=False,\n",
    "        )\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=early_stopping_patience,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            num_sanity_val_steps=0,\n",
    "            # log_every_n_steps=0,\n",
    "            callbacks=[\n",
    "                f1_checkpoint_callback,\n",
    "                early_stopping_callback\n",
    "            ],\n",
    "            default_root_dir=logs_folder,\n",
    "            logger=wandb_logger,\n",
    "        )\n",
    "\n",
    "        trainer.fit(graph_model, datamodule=data_module)\n",
    "\n",
    "        test_results = []\n",
    "        test_elapsed = []\n",
    "        print(\n",
    "            f\"==>> f1_checkpoint_callback.best_k_models.keys(): {f1_checkpoint_callback.best_k_models.keys()}\")\n",
    "        for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n",
    "            graph_model.test_prefix = f\"best_f1_{i}\"\n",
    "            results = trainer.test(\n",
    "                graph_model, datamodule=data_module, ckpt_path=k, verbose=False)\n",
    "            test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n",
    "            test_elapsed.append(results[0][f\"best_f1_{i}_elapsed\"])\n",
    "\n",
    "        logs = {\n",
    "            \"median_f1_of_best_f1\": np.median(test_results),\n",
    "            \"max_f1_of_best_f1\": np.max(test_results),\n",
    "            \"avg_f1_of_best_f1\": np.mean(test_results)\n",
    "        }\n",
    "        elapsed[model_name] = np.mean(test_elapsed).item()\n",
    "        print(f\"==>> model_name: {model_name}\")\n",
    "        print(f\"==>> test_elapsed: {np.mean(test_elapsed)}\")\n",
    "        if using_wandb:\n",
    "            wandb.log(logs)\n",
    "            wandb.finish()\n",
    "        else:\n",
    "            trainer.logger.log_metrics(logs, step=trainer.global_step)\n",
    "\n",
    "    time_elapsed_dict[dataset.name] = elapsed\n",
    "\n",
    "print(f\"==>> time_elapsed_dict: {time_elapsed_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the nested dictionary into a DataFrame\n",
    "df = pd.DataFrame.from_dict(time_elapsed_dict, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average time for each model (i.e.,column-wise mean)\n",
    "average_times = df.mean(axis=0)\n",
    "average_times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
