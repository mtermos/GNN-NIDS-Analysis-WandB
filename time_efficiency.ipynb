{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import psutil\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "import wandb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from src.models import EGAT, EGRAPHSAGE\n",
    "# from src.models import EGAT, EGCN, EGRAPHSAGE\n",
    "from src.lightning_model import GraphModel\n",
    "from src.lightning_data import GraphDataModule\n",
    "from src.dataset.dataset_info import datasets\n",
    "from local_variables import local_datasets_path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_wandb = False\n",
    "save_top_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datasets = [\n",
    "    datasets[\"cic_ton_iot\"],\n",
    "    datasets[\"cic_ids_2017\"],\n",
    "    datasets[\"cic_ton_iot_modified\"],\n",
    "    datasets[\"ccd_inid_modified\"],\n",
    "    datasets[\"nf_uq_nids_modified\"],\n",
    "    datasets[\"edge_iiot\"],\n",
    "    datasets[\"nf_cse_cic_ids2018\"],\n",
    "    datasets[\"nf_uq_nids\"],\n",
    "    datasets[\"x_iiot\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = psutil.Process(os.getpid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = max_epochs = 1\n",
    "# early_stopping_patience = 20\n",
    "learning_rate = 0.005\n",
    "weight_decay = 0.0\n",
    "ndim_out = [128, 128]\n",
    "num_layers = 2\n",
    "number_neighbors = [25, 10]\n",
    "activation = F.relu\n",
    "dropout = 0.0\n",
    "residual = True\n",
    "multi_class = True\n",
    "use_centralities_nfeats = False\n",
    "sort_timestamp = False\n",
    "aggregation = \"mean\"\n",
    "\n",
    "run_dtime = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "graph_type = \"flow\"\n",
    "# graph_type = \"window\"\n",
    "batch_size = 1\n",
    "# graph_type = \"line\"\n",
    "\n",
    "window_size = 1000\n",
    "\n",
    "g_type = \"\"\n",
    "if graph_type == \"flow\":\n",
    "    g_type = \"flow\"\n",
    "elif graph_type == \"window\":\n",
    "    g_type = f\"window_graph_{window_size}\"\n",
    "\n",
    "if multi_class:\n",
    "    g_type += \"__multi_class\"\n",
    "\n",
    "if use_centralities_nfeats:\n",
    "    g_type += \"__n_feats\"\n",
    "\n",
    "if sort_timestamp:\n",
    "    g_type += \"__sorted\"\n",
    "else:\n",
    "    g_type += \"__unsorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 33.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "33.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.8 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.13it/s, v_num=15, val_loss=2.130, val_acc=15.00, val_f1_score=19.30, train_loss=2.320, train_acc=5.290, train_f1_score=8.630]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.13it/s, v_num=15, val_loss=2.130, val_acc=15.00, val_f1_score=19.30, train_loss=2.320, train_acc=5.290, train_f1_score=8.630]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot\\\\lightning_logs\\\\version_15\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=19.26.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot\\lightning_logs\\version_15\\checkpoints\\best-val-f1-epoch=00-val_f1_score=19.26.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot\\lightning_logs\\version_15\\checkpoints\\best-val-f1-epoch=00-val_f1_score=19.26.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.3457060009241104\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:18<00:00,  0.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.3457059860229492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 33.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "33.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.8 K    Total params\n",
      "0.135     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:06<00:00,  0.17it/s, v_num=16, val_loss=2.100, val_acc=18.80, val_f1_score=27.60, train_loss=2.330, train_acc=7.880, train_f1_score=11.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:06<00:00,  0.17it/s, v_num=16, val_loss=2.100, val_acc=18.80, val_f1_score=27.60, train_loss=2.330, train_acc=7.880, train_f1_score=11.00]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot\\\\lightning_logs\\\\version_16\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=27.60.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot\\lightning_logs\\version_16\\checkpoints\\best-val-f1-epoch=00-val_f1_score=27.60.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot\\lightning_logs\\version_16\\checkpoints\\best-val-f1-epoch=00-val_f1_score=27.60.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.3931763023138046\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:18<00:00,  0.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.3931763172149658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 34.1 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "34.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "34.1 K    Total params\n",
      "0.136     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:31<00:00,  0.03it/s, v_num=17, val_loss=2.170, val_acc=10.30, val_f1_score=13.60, train_loss=2.370, train_acc=0.841, train_f1_score=1.520]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:31<00:00,  0.03it/s, v_num=17, val_loss=2.170, val_acc=10.30, val_f1_score=13.60, train_loss=2.370, train_acc=0.841, train_f1_score=1.520]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot\\\\lightning_logs\\\\version_17\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=13.64.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot\\lightning_logs\\version_17\\checkpoints\\best-val-f1-epoch=00-val_f1_score=13.64.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot\\lightning_logs\\version_17\\checkpoints\\best-val-f1-epoch=00-val_f1_score=13.64.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 3.666096404194832\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:21<00:00,  0.05it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 3.6660964488983154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 32.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "32.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "32.9 K    Total params\n",
      "0.131     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.30it/s, v_num=11, val_loss=2.630, val_acc=34.60, val_f1_score=46.10, train_loss=2.960, train_acc=3.440, train_f1_score=6.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.30it/s, v_num=11, val_loss=2.630, val_acc=34.60, val_f1_score=46.10, train_loss=2.960, train_acc=3.440, train_f1_score=6.000]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ids_2017\\\\lightning_logs\\\\version_11\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=46.12.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ids_2017\\lightning_logs\\version_11\\checkpoints\\best-val-f1-epoch=00-val_f1_score=46.12.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ids_2017\\lightning_logs\\version_11\\checkpoints\\best-val-f1-epoch=00-val_f1_score=46.12.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.17475400120019913\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:16<00:00,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.17475399374961853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 32.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "32.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "32.9 K    Total params\n",
      "0.131     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.30it/s, v_num=12, val_loss=2.530, val_acc=37.00, val_f1_score=51.30, train_loss=2.890, train_acc=18.40, train_f1_score=29.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.30it/s, v_num=12, val_loss=2.530, val_acc=37.00, val_f1_score=51.30, train_loss=2.890, train_acc=18.40, train_f1_score=29.10]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ids_2017\\\\lightning_logs\\\\version_12\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=51.28.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ids_2017\\lightning_logs\\version_12\\checkpoints\\best-val-f1-epoch=00-val_f1_score=51.28.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ids_2017\\lightning_logs\\version_12\\checkpoints\\best-val-f1-epoch=00-val_f1_score=51.28.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.20365899801254272\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:16<00:00,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.20365899801254272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 33.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "33.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.2 K    Total params\n",
      "0.133     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:41<00:00,  0.02it/s, v_num=13, val_loss=2.580, val_acc=41.30, val_f1_score=54.70, train_loss=2.680, train_acc=3.110, train_f1_score=3.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:41<00:00,  0.02it/s, v_num=13, val_loss=2.580, val_acc=41.30, val_f1_score=54.70, train_loss=2.680, train_acc=3.110, train_f1_score=3.810]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ids_2017\\\\lightning_logs\\\\version_13\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=54.74.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ids_2017\\lightning_logs\\version_13\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.74.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ids_2017\\lightning_logs\\version_13\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.74.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 5.1143684014678\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:21<00:00,  0.05it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 5.114368438720703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 31.0 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "31.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 K    Total params\n",
      "0.124     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:04<00:00,  0.21it/s, v_num=10, val_loss=1.630, val_acc=13.80, val_f1_score=16.40, train_loss=1.790, train_acc=6.550, train_f1_score=9.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:04<00:00,  0.21it/s, v_num=10, val_loss=1.630, val_acc=13.80, val_f1_score=16.40, train_loss=1.790, train_acc=6.550, train_f1_score=9.410]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot_modified\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=16.41.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot_modified\\lightning_logs\\version_10\\checkpoints\\best-val-f1-epoch=00-val_f1_score=16.41.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot_modified\\lightning_logs\\version_10\\checkpoints\\best-val-f1-epoch=00-val_f1_score=16.41.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.30604270100593567\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:17<00:00,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.30604270100593567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 31.0 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "31.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.0 K    Total params\n",
      "0.124     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  0.19it/s, v_num=11, val_loss=1.670, val_acc=40.60, val_f1_score=43.70, train_loss=1.910, train_acc=32.10, train_f1_score=29.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:05<00:00,  0.19it/s, v_num=11, val_loss=1.670, val_acc=40.60, val_f1_score=43.70, train_loss=1.910, train_acc=32.10, train_f1_score=29.80]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot_modified\\\\lightning_logs\\\\version_11\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=43.66.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot_modified\\lightning_logs\\version_11\\checkpoints\\best-val-f1-epoch=00-val_f1_score=43.66.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot_modified\\lightning_logs\\version_11\\checkpoints\\best-val-f1-epoch=00-val_f1_score=43.66.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.34326600283384323\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:16<00:00,  0.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.34326601028442383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 31.3 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "31.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.3 K    Total params\n",
      "0.125     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:27<00:00,  0.04it/s, v_num=12, val_loss=1.670, val_acc=46.20, val_f1_score=54.90, train_loss=1.910, train_acc=2.730, train_f1_score=3.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:27<00:00,  0.04it/s, v_num=12, val_loss=1.670, val_acc=46.20, val_f1_score=54.90, train_loss=1.910, train_acc=2.730, train_f1_score=3.420]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\cic_ton_iot_modified\\\\lightning_logs\\\\version_12\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=54.95.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\cic_ton_iot_modified\\lightning_logs\\version_12\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.95.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\cic_ton_iot_modified\\lightning_logs\\version_12\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.95.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 3.266692705452442\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:20<00:00,  0.05it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 3.266692638397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 27.5 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "27.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "27.5 K    Total params\n",
      "0.110     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, v_num=26, val_loss=1.710, val_acc=57.50, val_f1_score=60.30, train_loss=1.800, train_acc=27.20, train_f1_score=30.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  4.51it/s, v_num=26, val_loss=1.710, val_acc=57.50, val_f1_score=60.30, train_loss=1.800, train_acc=27.20, train_f1_score=30.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\ccd_inid_modified\\lightning_logs\\version_26\\checkpoints\\best-val-f1-epoch=00-val_f1_score=60.30.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\ccd_inid_modified\\\\lightning_logs\\\\version_26\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=60.30.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at logs\\ccd_inid_modified\\lightning_logs\\version_26\\checkpoints\\best-val-f1-epoch=00-val_f1_score=60.30.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.012954294681549072\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.012954294681549072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 27.5 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "27.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "27.5 K    Total params\n",
      "0.110     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  6.76it/s, v_num=27, val_loss=1.690, val_acc=60.50, val_f1_score=59.60, train_loss=1.780, train_acc=25.00, train_f1_score=26.00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  6.58it/s, v_num=27, val_loss=1.690, val_acc=60.50, val_f1_score=59.60, train_loss=1.780, train_acc=25.00, train_f1_score=26.00]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\ccd_inid_modified\\\\lightning_logs\\\\version_27\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=59.58.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\ccd_inid_modified\\lightning_logs\\version_27\\checkpoints\\best-val-f1-epoch=00-val_f1_score=59.58.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\ccd_inid_modified\\lightning_logs\\version_27\\checkpoints\\best-val-f1-epoch=00-val_f1_score=59.58.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.021133802831172943\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.021133802831172943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 27.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "27.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "27.8 K    Total params\n",
      "0.111     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.50it/s, v_num=28, val_loss=1.700, val_acc=55.40, val_f1_score=56.40, train_loss=1.790, train_acc=10.00, train_f1_score=10.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.50it/s, v_num=28, val_loss=1.700, val_acc=55.40, val_f1_score=56.40, train_loss=1.790, train_acc=10.00, train_f1_score=10.20]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\ccd_inid_modified\\\\lightning_logs\\\\version_28\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=56.42.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\ccd_inid_modified\\lightning_logs\\version_28\\checkpoints\\best-val-f1-epoch=00-val_f1_score=56.42.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\ccd_inid_modified\\lightning_logs\\version_28\\checkpoints\\best-val-f1-epoch=00-val_f1_score=56.42.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.5526880994439125\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 0.5526881217956543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 22.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "22.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.35it/s, v_num=6, val_loss=2.410, val_acc=8.320, val_f1_score=14.70, train_loss=2.510, train_acc=10.90, train_f1_score=18.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.35it/s, v_num=6, val_loss=2.410, val_acc=8.320, val_f1_score=14.70, train_loss=2.510, train_acc=10.90, train_f1_score=18.60]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids_modified\\\\lightning_logs\\\\version_6\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=14.73.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids_modified\\lightning_logs\\version_6\\checkpoints\\best-val-f1-epoch=00-val_f1_score=14.73.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids_modified\\lightning_logs\\version_6\\checkpoints\\best-val-f1-epoch=00-val_f1_score=14.73.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.16071169823408127\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:09<00:00,  0.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.16071170568466187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 22.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "22.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.34it/s, v_num=7, val_loss=2.380, val_acc=47.10, val_f1_score=54.70, train_loss=2.520, train_acc=0.214, train_f1_score=0.101]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:02<00:00,  0.34it/s, v_num=7, val_loss=2.380, val_acc=47.10, val_f1_score=54.70, train_loss=2.520, train_acc=0.214, train_f1_score=0.101]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids_modified\\\\lightning_logs\\\\version_7\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=54.75.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids_modified\\lightning_logs\\version_7\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.75.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids_modified\\lightning_logs\\version_7\\checkpoints\\best-val-f1-epoch=00-val_f1_score=54.75.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.1268210932612419\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:08<00:00,  0.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.1268211007118225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 23.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "23.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.2 K    Total params\n",
      "0.093     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=8, val_loss=2.380, val_acc=5.170, val_f1_score=5.430, train_loss=2.520, train_acc=0.279, train_f1_score=0.102]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s, v_num=8, val_loss=2.380, val_acc=5.170, val_f1_score=5.430, train_loss=2.520, train_acc=0.279, train_f1_score=0.102]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids_modified\\\\lightning_logs\\\\version_8\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=5.43.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids_modified\\lightning_logs\\version_8\\checkpoints\\best-val-f1-epoch=00-val_f1_score=5.43.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids_modified\\lightning_logs\\version_8\\checkpoints\\best-val-f1-epoch=00-val_f1_score=5.43.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 1.5403748974204063\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:10<00:00,  0.10it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 1.5403748750686646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 36.5 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.5 K    Total params\n",
      "0.146     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=3, val_loss=2.620, val_acc=7.480, val_f1_score=9.700, train_loss=3.050, train_acc=4.990, train_f1_score=8.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=3, val_loss=2.620, val_acc=7.480, val_f1_score=9.700, train_loss=3.050, train_acc=4.990, train_f1_score=8.050]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\edge_iiot\\\\lightning_logs\\\\version_3\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=9.70.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\edge_iiot\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=9.70.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\edge_iiot\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=9.70.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.13440439850091934\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:10<00:00,  0.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.13440439105033875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 36.5 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.5 K    Total params\n",
      "0.146     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=4, val_loss=2.620, val_acc=7.920, val_f1_score=9.740, train_loss=3.120, train_acc=0.776, train_f1_score=0.465]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:03<00:00,  0.31it/s, v_num=4, val_loss=2.620, val_acc=7.920, val_f1_score=9.740, train_loss=3.120, train_acc=0.776, train_f1_score=0.465]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\edge_iiot\\\\lightning_logs\\\\version_4\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=9.74.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\edge_iiot\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=9.74.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\edge_iiot\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=9.74.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.15739620476961136\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:10<00:00,  0.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.15739619731903076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 36.8 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.8 K    Total params\n",
      "0.147     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.13it/s, v_num=5, val_loss=2.570, val_acc=13.50, val_f1_score=12.80, train_loss=3.010, train_acc=3.500, train_f1_score=3.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.13it/s, v_num=5, val_loss=2.570, val_acc=13.50, val_f1_score=12.80, train_loss=3.010, train_acc=3.500, train_f1_score=3.720]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\edge_iiot\\\\lightning_logs\\\\version_5\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=12.77.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\edge_iiot\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=12.77.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\edge_iiot\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=12.77.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.6823584958910942\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:11<00:00,  0.09it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 0.6823585033416748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 22.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "22.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.14it/s, v_num=3, val_loss=2.650, val_acc=0.766, val_f1_score=0.494, train_loss=3.000, train_acc=0.555, train_f1_score=0.833]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.14it/s, v_num=3, val_loss=2.650, val_acc=0.766, val_f1_score=0.494, train_loss=3.000, train_acc=0.555, train_f1_score=0.833]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_cse_cic_ids2018\\\\lightning_logs\\\\version_3\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=0.49.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=0.49.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=0.49.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.4179924950003624\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:43<00:00,  0.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.417992502450943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 22.9 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "22.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.9 K    Total params\n",
      "0.092     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.14it/s, v_num=4, val_loss=2.660, val_acc=3.360, val_f1_score=4.050, train_loss=3.090, train_acc=0.179, train_f1_score=0.289]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:07<00:00,  0.14it/s, v_num=4, val_loss=2.660, val_acc=3.360, val_f1_score=4.050, train_loss=3.090, train_acc=0.179, train_f1_score=0.289]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_cse_cic_ids2018\\\\lightning_logs\\\\version_4\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=4.05.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=4.05.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=4.05.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.47491469979286194\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:43<00:00,  0.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.47491469979286194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 23.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "23.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.2 K    Total params\n",
      "0.093     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [01:45<00:00,  0.01it/s, v_num=5, val_loss=2.380, val_acc=22.40, val_f1_score=35.20, train_loss=2.750, train_acc=25.60, train_f1_score=39.80]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [01:45<00:00,  0.01it/s, v_num=5, val_loss=2.380, val_acc=22.40, val_f1_score=35.20, train_loss=2.750, train_acc=25.60, train_f1_score=39.80]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_cse_cic_ids2018\\\\lightning_logs\\\\version_5\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=35.22.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=35.22.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_cse_cic_ids2018\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=35.22.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 26.61699480563402\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [01:13<00:00,  0.01it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 26.616994857788086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 25.3 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:12<00:00,  0.08it/s, v_num=3, val_loss=2.950, val_acc=8.550, val_f1_score=3.750, train_loss=3.070, train_acc=4.850, train_f1_score=1.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:12<00:00,  0.08it/s, v_num=3, val_loss=2.950, val_acc=8.550, val_f1_score=3.750, train_loss=3.070, train_acc=4.850, train_f1_score=1.350]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids\\\\lightning_logs\\\\version_3\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=3.75.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=3.75.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids\\lightning_logs\\version_3\\checkpoints\\best-val-f1-epoch=00-val_f1_score=3.75.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.5543504059314728\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:44<00:00,  0.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.5543503761291504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 25.3 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "25.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.3 K    Total params\n",
      "0.101     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:14<00:00,  0.07it/s, v_num=4, val_loss=2.990, val_acc=23.80, val_f1_score=34.60, train_loss=3.110, train_acc=13.10, train_f1_score=21.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:14<00:00,  0.07it/s, v_num=4, val_loss=2.990, val_acc=23.80, val_f1_score=34.60, train_loss=3.110, train_acc=13.10, train_f1_score=21.10]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids\\\\lightning_logs\\\\version_4\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=34.63.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=34.63.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids\\lightning_logs\\version_4\\checkpoints\\best-val-f1-epoch=00-val_f1_score=34.63.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.5802456960082054\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:44<00:00,  0.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.5802456736564636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 25.6 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "25.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 K    Total params\n",
      "0.102     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [02:05<00:00,  0.01it/s, v_num=5, val_loss=3.000, val_acc=20.40, val_f1_score=27.70, train_loss=3.110, train_acc=12.70, train_f1_score=19.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [02:05<00:00,  0.01it/s, v_num=5, val_loss=3.000, val_acc=20.40, val_f1_score=27.70, train_loss=3.110, train_acc=12.70, train_f1_score=19.90]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\nf_uq_nids\\\\lightning_logs\\\\version_5\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=27.71.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\nf_uq_nids\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=27.71.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\nf_uq_nids\\lightning_logs\\version_5\\checkpoints\\best-val-f1-epoch=00-val_f1_score=27.71.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 26.428926907479763\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [01:10<00:00,  0.01it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 26.428926467895508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 36.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.2 K    Total params\n",
      "0.145     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.94it/s, v_num=16, val_loss=2.140, val_acc=26.50, val_f1_score=34.90, train_loss=2.310, train_acc=5.440, train_f1_score=8.460]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.94it/s, v_num=16, val_loss=2.140, val_acc=26.50, val_f1_score=34.90, train_loss=2.310, train_acc=5.440, train_f1_score=8.460]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\x_iiot\\\\lightning_logs\\\\version_16\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=34.94.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\x_iiot\\lightning_logs\\version_16\\checkpoints\\best-val-f1-epoch=00-val_f1_score=34.94.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\x_iiot\\lightning_logs\\version_16\\checkpoints\\best-val-f1-epoch=00-val_f1_score=34.94.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.04665200412273407\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:02<00:00,  0.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean\n",
      "==>> test_elapsed: 0.04665200412273407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGRAPHSAGE       | 36.2 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.2 K    Total params\n",
      "0.145     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.84it/s, v_num=17, val_loss=2.050, val_acc=32.60, val_f1_score=29.90, train_loss=2.270, train_acc=13.20, train_f1_score=18.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:01<00:00,  0.83it/s, v_num=17, val_loss=2.050, val_acc=32.60, val_f1_score=29.90, train_loss=2.270, train_acc=13.20, train_f1_score=18.40]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\x_iiot\\\\lightning_logs\\\\version_17\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=29.87.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\x_iiot\\lightning_logs\\version_17\\checkpoints\\best-val-f1-epoch=00-val_f1_score=29.87.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\x_iiot\\lightning_logs\\version_17\\checkpoints\\best-val-f1-epoch=00-val_f1_score=29.87.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.05002470314502716\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:02<00:00,  0.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>> model_name: e_graphsage_mean_no_sampling\n",
      "==>> test_elapsed: 0.05002470314502716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | EGAT             | 36.5 K | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "36.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.5 K    Total params\n",
      "0.146     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Administrateur\\Desktop\\GNN-NIDS-Analysis-WandB\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:04<00:00,  0.22it/s, v_num=18, val_loss=2.100, val_acc=20.10, val_f1_score=24.00, train_loss=2.330, train_acc=5.600, train_f1_score=6.710]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:04<00:00,  0.22it/s, v_num=18, val_loss=2.100, val_acc=20.10, val_f1_score=24.00, train_loss=2.330, train_acc=5.600, train_f1_score=6.710]\n",
      "==>> f1_checkpoint_callback.best_k_models.keys(): dict_keys(['logs\\\\x_iiot\\\\lightning_logs\\\\version_18\\\\checkpoints\\\\best-val-f1-epoch=00-val_f1_score=23.96.ckpt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs\\x_iiot\\lightning_logs\\version_18\\checkpoints\\best-val-f1-epoch=00-val_f1_score=23.96.ckpt\n",
      "Loaded model weights from the checkpoint at logs\\x_iiot\\lightning_logs\\version_18\\checkpoints\\best-val-f1-epoch=00-val_f1_score=23.96.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]==>> elapsed: 0.9356919080018997\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:03<00:00,  0.26it/s]\n",
      "==>> model_name: e_gat_no_sampling\n",
      "==>> test_elapsed: 0.9356918931007385\n",
      "==>> time_elapsed_dict: {'cic_ton_iot': {'e_graphsage_mean': 0.3457059860229492, 'e_graphsage_mean_no_sampling': 0.3931763172149658, 'e_gat_no_sampling': 3.6660964488983154}, 'cic_ids_2017': {'e_graphsage_mean': 0.17475399374961853, 'e_graphsage_mean_no_sampling': 0.20365899801254272, 'e_gat_no_sampling': 5.114368438720703}, 'cic_ton_iot_modified': {'e_graphsage_mean': 0.30604270100593567, 'e_graphsage_mean_no_sampling': 0.34326601028442383, 'e_gat_no_sampling': 3.266692638397217}, 'ccd_inid_modified': {'e_graphsage_mean': 0.012954294681549072, 'e_graphsage_mean_no_sampling': 0.021133802831172943, 'e_gat_no_sampling': 0.5526881217956543}, 'nf_uq_nids_modified': {'e_graphsage_mean': 0.16071170568466187, 'e_graphsage_mean_no_sampling': 0.1268211007118225, 'e_gat_no_sampling': 1.5403748750686646}, 'edge_iiot': {'e_graphsage_mean': 0.13440439105033875, 'e_graphsage_mean_no_sampling': 0.15739619731903076, 'e_gat_no_sampling': 0.6823585033416748}, 'nf_cse_cic_ids2018': {'e_graphsage_mean': 0.417992502450943, 'e_graphsage_mean_no_sampling': 0.47491469979286194, 'e_gat_no_sampling': 26.616994857788086}, 'nf_uq_nids': {'e_graphsage_mean': 0.5543503761291504, 'e_graphsage_mean_no_sampling': 0.5802456736564636, 'e_gat_no_sampling': 26.428926467895508}, 'x_iiot': {'e_graphsage_mean': 0.04665200412273407, 'e_graphsage_mean_no_sampling': 0.05002470314502716, 'e_gat_no_sampling': 0.9356918931007385}}\n"
     ]
    }
   ],
   "source": [
    "time_elapsed_dict = {}\n",
    "mem_usage_dict = {}\n",
    "\n",
    "for dataset in my_datasets:\n",
    "    dataset_folder = os.path.join(local_datasets_path, dataset.name)\n",
    "    graphs_folder = os.path.join(dataset_folder, g_type)\n",
    "\n",
    "    logs_folder = os.path.join(\"logs\", dataset.name)\n",
    "    os.makedirs(logs_folder, exist_ok=True)\n",
    "    wandb_runs_path = os.path.join(\"logs\", \"wandb_runs\")\n",
    "    os.makedirs(wandb_runs_path, exist_ok=True)\n",
    "\n",
    "    labels_mapping = {0: \"Normal\", 1: \"Attack\"}\n",
    "    num_classes = 2\n",
    "    if multi_class:\n",
    "        with open(os.path.join(dataset_folder, \"labels_names.pkl\"), \"rb\") as f:\n",
    "            labels_names = pickle.load(f)\n",
    "        labels_mapping = labels_names[0]\n",
    "    num_classes = len(labels_mapping)\n",
    "\n",
    "    dataset_kwargs = dict(\n",
    "        use_node_features=use_centralities_nfeats,\n",
    "        multi_class=True,\n",
    "        using_masking=False,\n",
    "        masked_class=2,\n",
    "        num_workers=0,\n",
    "        label_col=dataset.label_col,\n",
    "        class_num_col=dataset.class_num_col,\n",
    "        device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "    data_module = GraphDataModule(\n",
    "        graphs_folder, graph_type, batch_size=1, **dataset_kwargs)\n",
    "    data_module.setup()\n",
    "\n",
    "    ndim = next(iter(data_module.train_dataloader())).ndata[\"h\"].shape[-1]\n",
    "    edim = next(iter(data_module.train_dataloader())).edata['h'].shape[-1]\n",
    "\n",
    "    my_models = {\n",
    "        # \"e_gcn\": EGCN(ndim, edim, ndim_out, num_layers, activation,\n",
    "        #               dropout, residual, num_classes),\n",
    "        f\"e_graphsage_{aggregation}\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                                 residual, num_classes, num_neighbors=number_neighbors, aggregation=aggregation),\n",
    "        f\"e_graphsage_{aggregation}_no_sampling\": EGRAPHSAGE(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                                             residual, num_classes, num_neighbors=None, aggregation=aggregation),\n",
    "        \"e_gat_no_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "                                    residual, num_classes, num_neighbors=None),\n",
    "        # \"e_gat_sampling\": EGAT(ndim, edim, ndim_out, num_layers, activation, dropout,\n",
    "        #                        residual, num_classes, num_neighbors=number_neighbors),\n",
    "    }\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(data_module.train_dataset.class_weights)\n",
    "\n",
    "    elapsed = {}\n",
    "    mem_usage = {}\n",
    "\n",
    "    for model_name, model in my_models.items():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        peak_memory = 0\n",
    "\n",
    "        config = {\n",
    "            \"run_dtime\": run_dtime,\n",
    "            \"type\": \"GNN\",\n",
    "            \"model_name\": model_name,\n",
    "            \"max_epochs\": max_epochs,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"ndim\": ndim,\n",
    "            \"edim\": edim,\n",
    "            \"ndim_out\": ndim_out,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"number_neighbors\": number_neighbors,\n",
    "            \"activation\": activation.__name__,\n",
    "            \"dropout\": dropout,\n",
    "            \"residual\": residual,\n",
    "            \"multi_class\": multi_class,\n",
    "            \"aggregation\": aggregation,\n",
    "            # \"details\": \"updating edge features\",\n",
    "            \"early_stopping_patience\": early_stopping_patience,\n",
    "            \"use_centralities_nfeats\": use_centralities_nfeats,\n",
    "        }\n",
    "\n",
    "        graph_model = GraphModel(model, criterion, learning_rate, config, model_name,\n",
    "                                    labels_mapping, weight_decay=weight_decay, using_wandb=using_wandb, norm=False, multi_class=True, verbose=False)\n",
    "\n",
    "        if using_wandb:\n",
    "            wandb_logger = WandbLogger(\n",
    "                project=f\"GNN-Analysis-{dataset.name}\",\n",
    "                name=model_name,\n",
    "                config=config,\n",
    "                save_dir=wandb_runs_path\n",
    "            )\n",
    "        else:\n",
    "            wandb_logger = None\n",
    "\n",
    "        f1_checkpoint_callback = ModelCheckpoint(\n",
    "            monitor=\"val_f1_score\",\n",
    "            mode=\"max\",\n",
    "            filename=\"best-val-f1-{epoch:02d}-{val_f1_score:.2f}\",\n",
    "            save_top_k=save_top_k,\n",
    "            save_on_train_epoch_end=False,\n",
    "            verbose=False,\n",
    "        )\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            patience=early_stopping_patience,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            num_sanity_val_steps=0,\n",
    "            # log_every_n_steps=0,\n",
    "            callbacks=[\n",
    "                f1_checkpoint_callback,\n",
    "                early_stopping_callback\n",
    "            ],\n",
    "            default_root_dir=logs_folder,\n",
    "            logger=wandb_logger,\n",
    "        )\n",
    "\n",
    "        trainer.fit(graph_model, datamodule=data_module)\n",
    "\n",
    "        test_results = []\n",
    "        test_elapsed = []\n",
    "        print(\n",
    "            f\"==>> f1_checkpoint_callback.best_k_models.keys(): {f1_checkpoint_callback.best_k_models.keys()}\")\n",
    "        for i, k in enumerate(f1_checkpoint_callback.best_k_models.keys()):\n",
    "            graph_model.test_prefix = f\"best_f1_{i}\"\n",
    "            results = trainer.test(\n",
    "                graph_model, datamodule=data_module, ckpt_path=k, verbose=False)\n",
    "            mem_now = process.memory_info().rss / 1024 / 1024  # in MB\n",
    "            peak_memory = max(peak_memory, mem_now)\n",
    "            test_results.append(results[0][f\"best_f1_{i}_test_f1\"])\n",
    "            test_elapsed.append(results[0][f\"best_f1_{i}_elapsed\"])\n",
    "\n",
    "        logs = {\n",
    "            \"median_f1_of_best_f1\": np.median(test_results),\n",
    "            \"max_f1_of_best_f1\": np.max(test_results),\n",
    "            \"avg_f1_of_best_f1\": np.mean(test_results)\n",
    "        }\n",
    "        elapsed[model_name] = np.mean(test_elapsed).item()\n",
    "        mem_usage[model_name] = peak_memory\n",
    "        print(f\"==>> model_name: {model_name}\")\n",
    "        print(f\"==>> test_elapsed: {np.mean(test_elapsed)}\")\n",
    "        if using_wandb:\n",
    "            wandb.log(logs)\n",
    "            wandb.finish()\n",
    "        else:\n",
    "            trainer.logger.log_metrics(logs, step=trainer.global_step)\n",
    "\n",
    "    time_elapsed_dict[dataset.name] = elapsed\n",
    "    \n",
    "\n",
    "    mem_usage_dict[dataset.name] = mem_usage\n",
    "\n",
    "print(f\"==>> time_elapsed_dict: {time_elapsed_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e_graphsage_mean</th>\n",
       "      <th>e_graphsage_mean_no_sampling</th>\n",
       "      <th>e_gat_no_sampling</th>\n",
       "      <th>e_graphsage_mean_memory_MB</th>\n",
       "      <th>e_graphsage_mean_no_sampling_memory_MB</th>\n",
       "      <th>e_gat_no_sampling_memory_MB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cic_ton_iot</th>\n",
       "      <td>0.345706</td>\n",
       "      <td>0.393176</td>\n",
       "      <td>3.666096</td>\n",
       "      <td>3275.417969</td>\n",
       "      <td>2965.042969</td>\n",
       "      <td>3314.628906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cic_ids_2017</th>\n",
       "      <td>0.174754</td>\n",
       "      <td>0.203659</td>\n",
       "      <td>5.114368</td>\n",
       "      <td>2579.078125</td>\n",
       "      <td>2617.851562</td>\n",
       "      <td>2712.066406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cic_ton_iot_modified</th>\n",
       "      <td>0.306043</td>\n",
       "      <td>0.343266</td>\n",
       "      <td>3.266693</td>\n",
       "      <td>2665.382812</td>\n",
       "      <td>3010.746094</td>\n",
       "      <td>3030.972656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccd_inid_modified</th>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.552688</td>\n",
       "      <td>1645.660156</td>\n",
       "      <td>1661.386719</td>\n",
       "      <td>1656.949219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nf_uq_nids_modified</th>\n",
       "      <td>0.160712</td>\n",
       "      <td>0.126821</td>\n",
       "      <td>1.540375</td>\n",
       "      <td>2286.519531</td>\n",
       "      <td>2566.164062</td>\n",
       "      <td>2486.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge_iiot</th>\n",
       "      <td>0.134404</td>\n",
       "      <td>0.157396</td>\n",
       "      <td>0.682359</td>\n",
       "      <td>2902.039062</td>\n",
       "      <td>3033.359375</td>\n",
       "      <td>3241.714844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nf_cse_cic_ids2018</th>\n",
       "      <td>0.417993</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>26.616995</td>\n",
       "      <td>2707.246094</td>\n",
       "      <td>2715.402344</td>\n",
       "      <td>2807.449219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nf_uq_nids</th>\n",
       "      <td>0.554350</td>\n",
       "      <td>0.580246</td>\n",
       "      <td>26.428926</td>\n",
       "      <td>3430.597656</td>\n",
       "      <td>3744.269531</td>\n",
       "      <td>3478.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_iiot</th>\n",
       "      <td>0.046652</td>\n",
       "      <td>0.050025</td>\n",
       "      <td>0.935692</td>\n",
       "      <td>2592.039062</td>\n",
       "      <td>2601.929688</td>\n",
       "      <td>2615.734375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      e_graphsage_mean  e_graphsage_mean_no_sampling  \\\n",
       "cic_ton_iot                   0.345706                      0.393176   \n",
       "cic_ids_2017                  0.174754                      0.203659   \n",
       "cic_ton_iot_modified          0.306043                      0.343266   \n",
       "ccd_inid_modified             0.012954                      0.021134   \n",
       "nf_uq_nids_modified           0.160712                      0.126821   \n",
       "edge_iiot                     0.134404                      0.157396   \n",
       "nf_cse_cic_ids2018            0.417993                      0.474915   \n",
       "nf_uq_nids                    0.554350                      0.580246   \n",
       "x_iiot                        0.046652                      0.050025   \n",
       "\n",
       "                      e_gat_no_sampling  e_graphsage_mean_memory_MB  \\\n",
       "cic_ton_iot                    3.666096                 3275.417969   \n",
       "cic_ids_2017                   5.114368                 2579.078125   \n",
       "cic_ton_iot_modified           3.266693                 2665.382812   \n",
       "ccd_inid_modified              0.552688                 1645.660156   \n",
       "nf_uq_nids_modified            1.540375                 2286.519531   \n",
       "edge_iiot                      0.682359                 2902.039062   \n",
       "nf_cse_cic_ids2018            26.616995                 2707.246094   \n",
       "nf_uq_nids                    26.428926                 3430.597656   \n",
       "x_iiot                         0.935692                 2592.039062   \n",
       "\n",
       "                      e_graphsage_mean_no_sampling_memory_MB  \\\n",
       "cic_ton_iot                                      2965.042969   \n",
       "cic_ids_2017                                     2617.851562   \n",
       "cic_ton_iot_modified                             3010.746094   \n",
       "ccd_inid_modified                                1661.386719   \n",
       "nf_uq_nids_modified                              2566.164062   \n",
       "edge_iiot                                        3033.359375   \n",
       "nf_cse_cic_ids2018                               2715.402344   \n",
       "nf_uq_nids                                       3744.269531   \n",
       "x_iiot                                           2601.929688   \n",
       "\n",
       "                      e_gat_no_sampling_memory_MB  \n",
       "cic_ton_iot                           3314.628906  \n",
       "cic_ids_2017                          2712.066406  \n",
       "cic_ton_iot_modified                  3030.972656  \n",
       "ccd_inid_modified                     1656.949219  \n",
       "nf_uq_nids_modified                   2486.046875  \n",
       "edge_iiot                             3241.714844  \n",
       "nf_cse_cic_ids2018                    2807.449219  \n",
       "nf_uq_nids                            3478.031250  \n",
       "x_iiot                                2615.734375  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the nested dictionary into a DataFrame\n",
    "time_df = pd.DataFrame.from_dict(time_elapsed_dict, orient='index')\n",
    "mem_df = pd.DataFrame.from_dict(mem_usage_dict, orient='index')\n",
    "\n",
    "# Optional: rename memory columns to avoid overlap\n",
    "mem_df = mem_df.add_suffix('_memory_MB')\n",
    "\n",
    "# Merge both\n",
    "df = pd.concat([time_df, mem_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e_graphsage_mean                             0.239285\n",
       "e_graphsage_mean_no_sampling                 0.261182\n",
       "e_gat_no_sampling                            7.644910\n",
       "e_graphsage_mean_memory_MB                2675.997830\n",
       "e_graphsage_mean_no_sampling_memory_MB    2768.461372\n",
       "e_gat_no_sampling_memory_MB               2815.954861\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average time for each model (i.e.,column-wise mean)\n",
    "average_times = df.mean(axis=0)\n",
    "average_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e_graphsage_mean': {'cic_ton_iot': 0.3457059860229492,\n",
       "  'cic_ids_2017': 0.17475399374961853,\n",
       "  'cic_ton_iot_modified': 0.30604270100593567,\n",
       "  'ccd_inid_modified': 0.012954294681549072,\n",
       "  'nf_uq_nids_modified': 0.16071170568466187,\n",
       "  'edge_iiot': 0.13440439105033875,\n",
       "  'nf_cse_cic_ids2018': 0.417992502450943,\n",
       "  'nf_uq_nids': 0.5543503761291504,\n",
       "  'x_iiot': 0.04665200412273407},\n",
       " 'e_graphsage_mean_no_sampling': {'cic_ton_iot': 0.3931763172149658,\n",
       "  'cic_ids_2017': 0.20365899801254272,\n",
       "  'cic_ton_iot_modified': 0.34326601028442383,\n",
       "  'ccd_inid_modified': 0.021133802831172943,\n",
       "  'nf_uq_nids_modified': 0.1268211007118225,\n",
       "  'edge_iiot': 0.15739619731903076,\n",
       "  'nf_cse_cic_ids2018': 0.47491469979286194,\n",
       "  'nf_uq_nids': 0.5802456736564636,\n",
       "  'x_iiot': 0.05002470314502716},\n",
       " 'e_gat_no_sampling': {'cic_ton_iot': 3.6660964488983154,\n",
       "  'cic_ids_2017': 5.114368438720703,\n",
       "  'cic_ton_iot_modified': 3.266692638397217,\n",
       "  'ccd_inid_modified': 0.5526881217956543,\n",
       "  'nf_uq_nids_modified': 1.5403748750686646,\n",
       "  'edge_iiot': 0.6823585033416748,\n",
       "  'nf_cse_cic_ids2018': 26.616994857788086,\n",
       "  'nf_uq_nids': 26.428926467895508,\n",
       "  'x_iiot': 0.9356918931007385},\n",
       " 'e_graphsage_mean_memory_MB': {'cic_ton_iot': 3275.41796875,\n",
       "  'cic_ids_2017': 2579.078125,\n",
       "  'cic_ton_iot_modified': 2665.3828125,\n",
       "  'ccd_inid_modified': 1645.66015625,\n",
       "  'nf_uq_nids_modified': 2286.51953125,\n",
       "  'edge_iiot': 2902.0390625,\n",
       "  'nf_cse_cic_ids2018': 2707.24609375,\n",
       "  'nf_uq_nids': 3430.59765625,\n",
       "  'x_iiot': 2592.0390625},\n",
       " 'e_graphsage_mean_no_sampling_memory_MB': {'cic_ton_iot': 2965.04296875,\n",
       "  'cic_ids_2017': 2617.8515625,\n",
       "  'cic_ton_iot_modified': 3010.74609375,\n",
       "  'ccd_inid_modified': 1661.38671875,\n",
       "  'nf_uq_nids_modified': 2566.1640625,\n",
       "  'edge_iiot': 3033.359375,\n",
       "  'nf_cse_cic_ids2018': 2715.40234375,\n",
       "  'nf_uq_nids': 3744.26953125,\n",
       "  'x_iiot': 2601.9296875},\n",
       " 'e_gat_no_sampling_memory_MB': {'cic_ton_iot': 3314.62890625,\n",
       "  'cic_ids_2017': 2712.06640625,\n",
       "  'cic_ton_iot_modified': 3030.97265625,\n",
       "  'ccd_inid_modified': 1656.94921875,\n",
       "  'nf_uq_nids_modified': 2486.046875,\n",
       "  'edge_iiot': 3241.71484375,\n",
       "  'nf_cse_cic_ids2018': 2807.44921875,\n",
       "  'nf_uq_nids': 3478.03125,\n",
       "  'x_iiot': 2615.734375}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
